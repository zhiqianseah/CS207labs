{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Implement a URL fetcher using Beautiful Soup in the callback version. We will implement a similar one using coroutines on wednesday. \n",
    "\n",
    "The implimentation will extend the read_response method by parsing for URL's using `bs4` . Start by creating globals:\n",
    "```\n",
    "urls_todo = set(['/'])\n",
    "seen_urls = set(['/'])\n",
    "```\n",
    "\n",
    "then:\n",
    "\n",
    "```\n",
    "links = self.parse_links()#write this\n",
    "```\n",
    "(using self.response)\n",
    "\n",
    "Then use the set `difference` method  to add new links to `urls_todo` and recursively set up a `Fetcher` instance.\n",
    "\n",
    "Now update the `seen_urls` and `urls_todo` thus:\n",
    "```\n",
    "seen_urls.update(links)\n",
    "urls_todo.remove(self.url)\n",
    "if not urls_todo:\n",
    "    stopped = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "from bs4 import BeautifulSoup\n",
    "from selectors import DefaultSelector, EVENT_READ, EVENT_WRITE\n",
    "\n",
    "selector = DefaultSelector()\n",
    "class Fetcher:\n",
    "    def __init__(self, host, url, level = 0):\n",
    "        self.response = b''  # Empty array of bytes.\n",
    "        self.host = host\n",
    "        self.url = url\n",
    "        self.sock = None\n",
    "        self.level = level\n",
    "        \n",
    "    # Method on Fetcher class.\n",
    "    def fetch(self):\n",
    "        self.sock = socket.socket()\n",
    "        self.sock.setblocking(False)\n",
    "        try:\n",
    "            self.sock.connect((self.host, 80))\n",
    "        except BlockingIOError:\n",
    "            pass\n",
    "\n",
    "        # Register next callback.\n",
    "        selector.register(self.sock.fileno(),\n",
    "                          EVENT_WRITE,\n",
    "                          self.connected)\n",
    "\n",
    "    def connected(self, key, mask):\n",
    "        print('connected to:', self.url, flush=True)\n",
    "        selector.unregister(key.fd)\n",
    "        request = 'GET {} HTTP/1.0\\r\\nHost: {}\\r\\n\\r\\n'.format(self.url, self.host)\n",
    "        self.sock.send(request.encode('ascii'))\n",
    "\n",
    "        # Register the next callback.\n",
    "        selector.register(key.fd,\n",
    "                          EVENT_READ,\n",
    "                          self.read_response)\n",
    "        \n",
    "    def read_response(self, key, mask):\n",
    "        global stopped\n",
    "        \n",
    "        chunk = self.sock.recv(4096)  # USUALLY 4k chunk size, here small\n",
    "        if chunk:\n",
    "            #print(\"read chunk\", flush=True)\n",
    "            self.response += chunk\n",
    "        else:\n",
    "            print(\"all read\", flush=True)\n",
    "            links = self.parse_links()\n",
    "            selector.unregister(key.fd)  # Done reading.\n",
    "            print (\"Links in:\", self.url, \" are:\",links)         \n",
    "            \n",
    "            links.difference_update(seen_urls)\n",
    "            #print (links)\n",
    "            if self.level < 1:\n",
    "                for link in links:\n",
    "                    urls_todo.add(link)\n",
    "                    \n",
    "            if self.level < 1:\n",
    "                for link in links:\n",
    "                    urls_todo.add(link)\n",
    "                    fetcher = Fetcher(self.host, link, self.level+1)\n",
    "                    fetcher.fetch()\n",
    "\n",
    "            seen_urls.update(links)\n",
    "            urls_todo.remove(self.url)\n",
    "            #print (len(urls_todo), self.url, urls_todo)\n",
    "            #print (list(selector.get_map()))\n",
    "            if not urls_todo:\n",
    "                stopped = True\n",
    "            \n",
    "            \n",
    "    def parse_links(self):\n",
    "        soup = BeautifulSoup(self.response, \"lxml\");\n",
    "        #print (soup)\n",
    "        result = set([])\n",
    "        for link in soup.find_all('a'):\n",
    "            linkurl = link.get('href')\n",
    "            if linkurl is not None and linkurl.startswith('/'):\n",
    "                #print (linkurl)\n",
    "                result.add(linkurl)\n",
    "        #for x in range(3):\n",
    "        #    host = 'xkcd.com'\n",
    "        #    url = '/37'+str(x)+'/'\n",
    "        #    result.add(url)\n",
    "        #print (result)\n",
    "        return result\n",
    "            \n",
    "stopped = False\n",
    "\n",
    "def loop():\n",
    "    while not stopped:\n",
    "        events = selector.select()\n",
    "        for event_key, event_mask in events:\n",
    "            callback = event_key.data\n",
    "            callback(event_key, event_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to: /\n",
      "all read\n",
      "Links in: /  are: {'/about', '/', '/atom.xml', '/license.html', '/1660/', '//c.xkcd.com/random/comic/', '/archive', '/1/', '/rss.xml'}\n",
      "connected to: /atom.xml\n",
      "connected to: /about\n",
      "connected to: /license.html\n",
      "connected to: /archive\n",
      "connected to: //c.xkcd.com/random/comic/\n",
      "connected to: /1660/\n",
      "connected to: /1/\n",
      "connected to: /rss.xml\n",
      "all read\n",
      "Links in: /about  are: set()\n",
      "all read\n",
      "Links in: //c.xkcd.com/random/comic/  are: set()\n",
      "all read\n",
      "Links in: /rss.xml  are: set()\n",
      "all read\n",
      "Links in: /1660/  are: {'/about', '/', '/atom.xml', '/license.html', '//c.xkcd.com/random/comic/', '/archive', '/1659/', '/1/', '/1661/', '/rss.xml'}\n",
      "all read\n",
      "Links in: /atom.xml  are: set()\n",
      "all read\n",
      "Links in: /license.html  are: set()\n",
      "all read\n",
      "Links in: /archive  are: set()\n",
      "all read\n",
      "Links in: /1/  are: {'/about', '/', '/atom.xml', '/license.html', '/2/', '//c.xkcd.com/random/comic/', '/archive', '/1/', '/rss.xml'}\n"
     ]
    }
   ],
   "source": [
    "urls_todo = set(['/'])\n",
    "seen_urls = set(['/'])\n",
    "\n",
    "fetcher = Fetcher('xkcd.com','/')\n",
    "fetcher.fetch()\n",
    "loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
